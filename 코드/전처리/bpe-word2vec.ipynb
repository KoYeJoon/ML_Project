{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataframe/df500k.pkl\n",
      "/kaggle/input/dataframe/yelp_stopwords(3m).txt\n",
      "/kaggle/input/dataframe/eng_w2v2\n",
      "/kaggle/input/dataframe/yelp_stopwords(500k).txt\n",
      "/kaggle/input/dataframe/m.model\n",
      "/kaggle/input/dataframe/m.vocab\n",
      "/kaggle/input/dataframe/df3m.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/kaggle/input/dataframe/df500k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193441\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>2997391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2019486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>1681123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>1589470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>1502952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193436</th>\n",
       "      <td>vulgarized</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193437</th>\n",
       "      <td>pyrus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193438</th>\n",
       "      <td>spaccavento</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193439</th>\n",
       "      <td>ecg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193440</th>\n",
       "      <td>stargaze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193441 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                key    value\n",
       "0               the  2997391\n",
       "1               and  2019486\n",
       "2                 i  1681123\n",
       "3                to  1589470\n",
       "4                 a  1502952\n",
       "...             ...      ...\n",
       "193436   vulgarized        1\n",
       "193437        pyrus        1\n",
       "193438  spaccavento        1\n",
       "193439          ecg        1\n",
       "193440     stargaze        1\n",
       "\n",
       "[193441 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[]\n",
    "for line in df['text']:\n",
    "    X.append(line)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t= Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "count=t.word_counts\n",
    "print(len(count))\n",
    "\n",
    "sorted_count=sorted(count.items(), key=lambda item: item[1], reverse=True)\n",
    "word_data=pd.DataFrame(sorted_count)\n",
    "word_data= word_data.rename(columns={0: \"key\", 1:\"value\"})\n",
    "x=word_data[0:50]['key']\n",
    "y=word_data[0:50]['value']\n",
    "word_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 빈도수 분포 \n",
    "y2_l=len(word_data)\n",
    "list1=[]\n",
    "list2=[]\n",
    "index=0\n",
    "for i in range(y2_l): \n",
    "    cnt=0\n",
    "    while y2_l-index-1>=0 and i+1==word_data['value'][y2_l-index-1]:\n",
    "        cnt+=1\n",
    "        #print(y2_l-index-1,y2[y2_l-index-1],cnt)\n",
    "        index+=1\n",
    "    if(cnt!=0):\n",
    "        list1.append(i+1)\n",
    "        list2.append(cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>ammount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>23605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    count  ammount\n",
       "0       1    94092\n",
       "1       2    23605\n",
       "2       3    11541\n",
       "3       4     7302\n",
       "4       5     5208\n",
       "..    ...      ...\n",
       "95     96       74\n",
       "96     97       73\n",
       "97     98       65\n",
       "98     99       65\n",
       "99    100       48\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_data = pd.DataFrame(list(zip(list1, list2)), \n",
    "               columns =['count', 'ammount']) \n",
    "word_count_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180371 93.24341788969247\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for i in range(100):\n",
    "    total+=word_count_data['ammount'][i]\n",
    "print(total,total/y2_l*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=/kaggle/input/dataframe/yelp_stopwords(500k).txt --model_prefix=m --vocab_size=10000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0100020003000400050006000700080009000100001100012000130001400015000160001700018000190002000021000220002300024000250002600027000280002900030000310003200033000340003500036000370003800039000400004100042000430004400045000460004700048000490005000051000520005300054000550005600057000580005900060000610006200063000640006500066000670006800069000700007100072000730007400075000760007700078000790008000081000820008300084000850008600087000880008900090000910009200093000940009500096000970009800099000100000101000102000103000104000105000106000107000108000109000110000111000112000113000114000115000116000117000118000119000120000121000122000123000124000125000126000127000128000129000130000131000132000133000134000135000136000137000138000139000140000141000142000143000144000145000146000147000148000149000150000151000152000153000154000155000156000157000158000159000160000161000162000163000164000165000166000167000168000169000170000171000172000173000174000175000176000177000178000179000180000181000182000183000184000185000186000187000188000189000190000191000192000193000194000195000196000197000198000199000200000201000202000203000204000205000206000207000208000209000210000211000212000213000214000215000216000217000218000219000220000221000222000223000224000225000226000227000228000229000230000231000232000233000234000235000236000237000238000239000240000241000242000243000244000245000246000247000248000249000250000251000252000253000254000255000256000257000258000259000260000261000262000263000264000265000266000267000268000269000270000271000272000273000274000275000276000277000278000279000280000281000282000283000284000285000286000287000288000289000290000291000292000293000294000295000296000297000298000299000300000301000302000303000304000305000306000307000308000309000310000311000312000313000314000315000316000317000318000319000320000321000322000323000324000325000326000327000328000329000330000331000332000333000334000335000336000337000338000339000340000341000342000343000344000345000346000347000348000349000350000351000352000353000354000355000356000357000358000359000360000361000362000363000364000365000366000367000368000369000370000371000372000373000374000375000376000377000378000379000380000381000382000383000384000385000386000387000388000389000390000391000392000393000394000395000396000397000398000399000400000401000402000403000404000405000406000407000408000409000410000411000412000413000414000415000416000417000418000419000420000421000422000423000424000425000426000427000428000429000430000431000432000433000434000435000436000437000438000439000440000441000442000443000444000445000446000447000448000449000450000451000452000453000454000455000456000457000458000459000460000461000462000463000464000465000466000467000468000469000470000471000472000473000474000475000476000477000478000479000480000481000482000483000484000485000486000487000488000489000490000491000492000493000494000495000496000497000498000499000[['▁', 'I', '▁am', '▁actually', '▁horrifi', 'ed', '▁thi', 's', '▁place', '▁i', 's', '▁still', '▁in', '▁business', '.', '▁', 'M', 'y', '▁3', '▁year', '▁old', '▁son', '▁needed', '▁a', '▁haircut', '▁thi', 's', '▁past', '▁summer', '▁and', '▁the', '▁lure', '▁', 'of', '▁the', '▁$', '7', '▁kids', '▁cut', '▁signs', '▁got', '▁me', '▁in', '▁the', '▁door', '.', '▁', 'W', 'e', '▁ha', 'd', '▁to', '▁wait', '▁a', '▁few', '▁minutes', '▁as', '▁bo', 'th', '▁stylists', '▁we', 're', '▁working', '▁on', '▁people', '.', '▁', 'T', 'he', '▁decor', '▁in', '▁thi', 's', '▁place', '▁i', 's', '▁total', '▁garbage', '.', '▁', 'I', 't', '▁i', 's', '▁so', '▁tacky', '.', '▁', 'T', 'he', '▁sofa', '▁', 'they', '▁ha', 'd', '▁a', 't', '▁the', '▁time', '▁was', '▁a', '▁ple', 'a', 'ther', '▁sofa', '▁with', '▁giant', '▁holes', '▁in', '▁it', '.', '▁', 'A', 'nd', '▁my', '▁son', '▁noticed', '▁ant', 's', '▁crawling', '▁all', '▁over', '▁the', '▁floor', '▁and', '▁the', '▁furniture', '.', '▁', 'I', 't', '▁was', '▁disgusting', '▁and', '▁', 'I', '▁', 'should', '▁ha', 've', '▁walked', '▁out', '▁', 'then', '.', '▁', 'A', 'c', 'tu', 'ally', ',', '▁', 'I', '▁', 'should', '▁ha', 've', '▁turned', '▁around', '▁and', '▁walked', '▁out', '▁upon', '▁entering', '▁bu', 't', '▁', 'I', '▁di', 'd', 'n', \"'\", 't', '.', '▁', 'S', 'o', '▁the', '▁older', '▁black', '▁male', '▁stylist', '▁finish', 'es', '▁the', '▁haircut', '▁he', '▁was', '▁do', 'ing', '▁and', '▁it', \"'\", 's', '▁', 'our', '▁turn', '.', '▁', 'I', '▁tell', '▁hi', 'm', '▁', 'I', '▁want', '▁a', '▁', '#', '2', '▁clippe', 'r', '▁around', '▁the', '▁back', '▁and', '▁sides', '▁and', '▁', 'then', '▁hand', '▁cut', '▁the', '▁top', '▁in', 'to', '▁a', '▁standard', '▁boys', '▁cut', '.', '▁', 'R', 'e', 'ally', '▁freaking', '▁simple', ',', '▁right', '?', '▁', 'WRONG', '!', '▁', 'R', 'a', 'ther', '▁', 'than', '▁use', '▁the', '▁clippers', '▁and', '▁go', '▁up', '▁to', '▁actually', '▁cut', '▁the', '▁hair', ',', '▁he', '▁went', '▁down', '.', '▁', 'U', 's', 'ing', '▁it', '▁moving', '▁down', 'ward', '▁doesn', \"'\", 't', '▁cut', '▁hair', ',', '▁it', '▁jus', 't', '▁rub', 's', '▁again', 's', 't', '▁it', '.', '▁', 'H', 'ow', '▁do', 'es', '▁thi', 's', '▁man', '▁', 'who', '▁ha', 's', '▁an', '▁al', 'leg', 'ed', '▁co', 's', 'me', 't', 'ology', '▁license', '▁not', '▁know', '▁', 'how', '▁to', '▁use', '▁a', '▁set', '▁', 'of', '▁freaking', '▁clippers', '?', '?', '?', '▁', 'I', '▁realized', '▁almost', '▁immediately', '▁that', '▁he', '▁ha', 'd', '▁no', '▁idea', '▁wha', 't', '▁he', '▁was', '▁do', 'ing', '.', '▁', 'N', 'o', '▁idea', '▁a', 't', '▁all', '.', '▁', 'A', 'ft', 'er', '▁', 'about', '▁10', '▁minutes', '▁', 'of', '▁watching', '▁thi', 's', '▁guy', '▁stumble', '▁', 'through', '▁it', ',', '▁', 'I', '▁said', '▁', '\"', 'you', '▁know', '▁wha', 't', '?', '▁', 'T', 'ha', 't', \"'\", 's', '▁fine', '.', '\"', ',', '▁paid', '▁and', '▁left', '.', '▁', 'A', 'll', '▁', 'I', '▁wanted', '▁to', '▁do', '▁was', '▁get', '▁out', '▁', 'of', '▁that', '▁scum', 'my', '▁joint', '▁and', '▁take', '▁my', '▁son', '▁to', '▁a', '▁real', '▁haircut', '▁place', '.', '▁', 'B', 'o', 't', 'to', 'm', '▁line', ':', '▁', 'DO', '▁', 'NOT', '▁', 'GO', '▁', 'HERE', '.', '▁', 'RUN', '▁', 'THE', '▁', 'OTHER', '▁', 'WAY', '!', '!', '!', '!', '!'], ['▁', 'D', 'ism', 'al', ',', '▁lukewarm', ',', '▁def', 'ro', 's', 't', 'ed', '-', 'tasting', '▁', '\"T', 'ex', 'M', 'ex', '\"', '▁gl', 'op', ';', '▁', 'M', 'um', 'b', 'ly', ',', '▁un', 'eng', 'aged', '▁waiter', ';', '▁', 'C', 'lu', 'e', 'less', '▁manager', ',', '▁', 'who', '▁seeing', '▁us', '▁with', '▁barely', '▁nibble', 'd', '▁entrees', '▁on', '▁plates', '▁shove', 'd', '▁forward', '▁for', '▁pickup', ',', '▁thank', 'ed', '▁us', '▁per', 'funct', 'ori', 'ly', '▁for', '▁', 'our', '▁patron', 'age', ';', '▁', 'W', 'e', \"'\", 're', '▁fro', 'm', '▁the', '▁', 'T', 'ex', 'as', '▁', 'H', 'i', 'll', '▁', 'C', 'o', 'un', 'try', ';', '▁down', '▁there', ',', '▁we', '▁ja', 'il', '▁cri', 'tter', 's', '▁', 'who', '▁serve', '▁up', '▁grub', '▁thi', 's', '▁bad', ',', '▁for', '▁the', 'ir', '▁', 'own', '▁protection', '.', '▁', 'N', 'ever', ',', '▁never', ',', '▁', 'NEVER', '▁again', '▁(', 'B', 'ack', '▁to', '▁', 'Y', 'ard', '▁', 'H', 'ous', 'e', '▁for', '▁real', '▁food', ')'], ['▁', 'I', 'f', '▁', 'I', '▁could', '▁give', '▁less', '▁', 'than', '▁one', '▁star', ',', '▁that', '▁would', '▁ha', 've', '▁be', 'en', '▁my', '▁choice', '.', '▁', 'I', '▁rent', '▁a', '▁home', '▁and', '▁', 'P', 'er', '▁my', '▁lease', '▁agreement', '▁it', '▁i', 's', '▁', 'MY', '▁responsibility', '▁to', '▁pay', '▁the', 'ir', '▁', 'P', 'o', 'ol', '▁', 'S', 'er', 'vi', 'ce', '▁company', '.', '▁', 'W', 'i', 'th', 'in', '▁the', '▁last', '▁year', '▁', 'they', '▁changed', '▁to', '▁', 'P', 'o', 'ol', 'S', 'er', 'v', '.', '▁', 'I', '▁ha', 've', '▁ha', 'd', '▁major', '▁issues', '▁with', '▁new', '▁techs', '▁every', '▁week', ',', '▁never', '▁checking', '▁', 'PH', '▁balance', 's', ',', '▁cleaning', '▁the', '▁filter', ',', '▁and', '▁not', '▁showing', '▁up', '▁a', 't', '▁all', '▁2', '▁weeks', '▁in', '▁the', '▁past', '▁2', '▁months', '.', '▁', 'I', '▁ha', 've', '▁ha', 'd', '▁4', '▁different', '▁techs', '▁in', '▁the', '▁past', '▁4', '▁weeks', '.', '▁', 'I', '▁ha', 've', '▁emailed', '▁and', '▁called', '▁them', '▁and', '▁', 'they', '▁never', '▁respond', '▁back', '▁n', 'or', '▁even', '▁acknowledged', '▁my', '▁concerns', '▁or', '▁requests', '.', '▁', 'I', '▁can', 'not', '▁change', '▁companies', '▁bu', 't', '▁', 'I', \"'\", 'm', '▁required', '▁to', '▁still', '▁pay', '▁for', '▁lousy', '▁or', '▁no', '▁service', '.', '▁', 'A', 't', 't', 'ach', 'ed', '▁a', 're', '▁a', '▁couple', '▁pictures', '▁', 'of', '▁my', '▁pool', '▁recently', '▁due', '▁to', '▁one', '▁tech', '▁jus', 't', '▁di', 'd', 'n', \"'\", 't', '▁put', '▁an', 'y', '▁ch', 'l', 'or', 'ine', '▁in', '▁it', '▁a', 't', '▁all', '▁according', '▁to', '▁the', '▁tech', '▁', 'who', '▁came', '▁the', '▁following', '▁week', '▁to', '▁attempt', '▁to', '▁clean', '▁it', '▁up', '.', '▁', 'P', 'le', 'ase', '▁think', '▁twice', '▁be', 'for', 'e', '▁working', '▁with', '▁th', 'ese', '▁people', '.', '▁', 'N', 'o', '▁one', '▁wants', '▁to', '▁work', '▁with', '▁a', '▁business', '▁that', '▁doesn', \"'\", 't', '▁return', '▁phone', '▁calls', '▁or', '▁emails', '.'], ['▁10', 'pm', '▁on', '▁a', '▁super', '▁bowl', '▁', 'S', 'un', 'day', '▁and', '▁', 'they', \"'\", 're', '▁already', '▁closed', '?', '?', '▁', 'W', 'e', 'ak', ',', '▁no', '▁wonder', '▁the', '▁hard', '▁', 'R', 'ock', '▁i', 's', '▁dying', '▁off', '.', '..'], ['▁', 'T', 'ri', 'ed', '▁to', '▁ha', 've', '▁my', '▁car', '▁repaired', '.', '▁', 'E', 've', 'n', '▁made', '▁an', '▁appointment', '.', '▁', 'I', '▁was', '▁told', '▁that', '▁all', '▁my', '▁appointment', '▁was', '▁for', '▁i', 's', '▁to', '▁meet', '▁the', '▁service', '▁advisor', ',', '▁not', '▁get', '▁an', 'y', '▁work', '▁completed', '▁on', '▁my', '▁car', '.', '▁', 'S', 'uch', '▁a', '▁waste', '▁', 'of', '▁time', '.', '▁', 'E', 've', 'n', '▁talked', '▁to', '▁the', '▁', 'S', 'er', 'vi', 'ce', '▁', 'M', 'ana', 'ger', ',', '▁he', '▁di', 'd', 'n', \"'\", 't', '▁care', '.', '▁', 'S', 'in', 'ce', '▁that', '▁was', '▁the', '▁attitude', '▁', 'of', '▁the', '▁service', '▁department', ',', '▁', 'I', '▁can', '▁on', 'ly', '▁expect', '▁that', '▁fro', 'm', '▁the', '▁mechanics', '▁that', '▁would', '▁ha', 've', '▁worked', '▁on', '▁my', '▁car', '.', '▁', 'W', 'ent', '▁to', '▁another', '▁dealership', ',', '▁made', '▁an', '▁appointment', '▁and', '▁my', '▁car', '▁went', '▁in', '▁to', '▁be', '▁repaired', '.', '▁', 'N', 'o', 't', '▁told', '▁to', '▁come', '▁back', '▁later', '▁and', '▁', 'they', '▁might', '▁get', '▁to', '▁my', '▁car', '.', '▁', 'V', 'ery', '▁poor', '▁service', '.']]\n"
     ]
    }
   ],
   "source": [
    "#import sentencepiece as spm\n",
    "#sp = spm.SentencePieceProcessor()\n",
    "#sp.load('/kaggle/input/dataframe/m.model')\n",
    "X2=[]\n",
    "for i, line in enumerate(df['text']):\n",
    "    if i%1000==0:\n",
    "        print(i,end='')\n",
    "    X2.append(sp.encode_as_pieces(line))\n",
    "\n",
    "print(X2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=X2, size=50, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁woman', 0.926487147808075), ('▁lady', 0.8885421752929688), ('▁guy', 0.8851172924041748), ('▁gentleman', 0.8785795569419861), ('▁girl', 0.8782986402511597), ('▁worker', 0.8658108115196228), ('▁gentlemen', 0.8455085158348083), ('▁dude', 0.8162292242050171), ('▁bouncer', 0.7841111421585083), ('▁gal', 0.7780405879020691)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"▁man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v2') # 모델 저장\n",
    "#loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
